{
  "comment": "Configuration for Data Lakes with Kafka, Snowflake, and Spark",
  "tech_stack": "data_lakes",
  "problem_commands": {
    "python_check": ["python", "-m", "py_compile"],
    "scala_check": ["scalac", "-Ystop-after:parser"]
  },
  "best_practices_files": ["./best_practices_guides/DATA_LAKES_KAFKA_SNOWFLAKE_SPARK_BEST_PRACTICES.md"],
  "chunk_size": 50,
  "agents": 20,
  "session": "claude_agents",
  "stagger": 10.0,
  "wait_after_cc": 15.0,
  "check_interval": 10,
  "skip_regenerate": false,
  "skip_commit": false,
  "auto_restart": true,
  "no_monitor": false,
  "attach": false,
  "prompt_file": "prompts/default_best_practices_prompt_data_lakes.txt",
  "context_threshold": 20,
  "idle_timeout": 60,
  "max_errors": 3,
  "git_branch": "main",
  "git_remote": "origin",
  "tmux_kill_on_exit": true,
  "tmux_mouse": true,
  "fast_start": false,
  "full_backup": false,
  "max_agents": 50
} 